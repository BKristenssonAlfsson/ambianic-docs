{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Helpful AI for Home and Business Automation Ambianic's mission is to make our homes and workspaces a little cozier. Ambianic is an Open Source Ambient Intelligence platform that puts local control and privacy first. It enables users to train and share custom AI models without compromising privacy. View on Github The 5 Whys What's the root cause for Ambianic.ai to exist? Below is a 5 Whys diagram that tries to provide objective answers: Needless to say there are subjective reasons which are equally if not more influential for the existence of this project such as basic human excitement to serve a bigger purpose via open source AI. User Journey Ambianic's roadmap is inspired by user stories and community feedback. The following diagram illustrates an example user journey. User journeys help us align on the bigger picture and segue into agile development constructs such as user stories and sprints. More user journeys will be added over time as the project evolves. Some of the candidate topics include: Home Automation Turn traditional door locks into smart locks with Face Recognition. Alert parents if a crying toddler is left unattended for more than 15 minutes. Raise an alert if a baby is seated near a car door without child lock enabled while in motion. Business Automation Prevent accidents by alerting drivers who act sleepy or distracted. Make sure that a factory floor position is not left unattended for more than 15 minutes. Recognize presence of unauthorized people in a restricted access work area. User - System Interactions Users interact with the system in two phases: First Time Installation Consequent App Engagements The following diagram illustrates the high level user - system interactions. User Interface Flow The User Interface is centered around three main activities: Setup Ambianic Edge to communicate with smart home devices: sensors, cameras, microphones, lights, door locks, and others. Design flows to automatically observe sensors and make helpful recommendations. Review event timeline, alerts and actionable recommendations. Ambianic UI is an Offline-First PWA (Progressive Web Application). PWAs work in any browser, but \"app-like\" with features such as being independent of connectivity, install to home screen, and push messaging depend on browser support. Ambianic UI does not assume that the user has constant broadband internet access. Its built to handle a range of real world scenarios with low bandwidth or no-Internet access at all when the user may need to review Ambianic alerts, events timeline, edit flows and configure edge devices. Ambianic UI stores data locally on the client device (mobile or desktop) and, when there\u2019s a network connection, syncs data to the user's Ambianic server and resolves any data conflicts. When possible it communicates directly with local Ambianic Edge devices minimizing network routing overhead. Project Status At this time, Ambianic is in active early formation stages. Design and implementation decisions are made daily with focus on advancing the project to an initial stable version as soon as possible. If you are willing to take the risk that comes with early stage code and are able to dive deep into Python, Javascript, Gstreamer, and Tensorflow code, then please keep reading. Otherwise you can register to watch for new releases . We will notify you as soon as a stable release is out. Product Design Goals Our goal is to build a product that is useful out of the box: Less than 15 minutes setup time Less than $75 in hardware costs Primary platform: Raspberry Pi 4 B, 4GB RAM, 32GB SDRAM No coding required to get started Decomposable and hackable for open source developers Quick Start If you would like to try the latest version, follow the steps in the Quick Start Guide . Contributors If you are interested in becoming a contributor to the project, please read the Contributing page and follow the steps. Looking forward to hearing from you!","title":"Home"},{"location":"#helpful-ai-for-home-and-business-automation","text":"Ambianic's mission is to make our homes and workspaces a little cozier. Ambianic is an Open Source Ambient Intelligence platform that puts local control and privacy first. It enables users to train and share custom AI models without compromising privacy. View on Github","title":"Helpful AI for Home and Business Automation"},{"location":"#the-5-whys","text":"What's the root cause for Ambianic.ai to exist? Below is a 5 Whys diagram that tries to provide objective answers: Needless to say there are subjective reasons which are equally if not more influential for the existence of this project such as basic human excitement to serve a bigger purpose via open source AI.","title":"The 5 Whys"},{"location":"#user-journey","text":"Ambianic's roadmap is inspired by user stories and community feedback. The following diagram illustrates an example user journey. User journeys help us align on the bigger picture and segue into agile development constructs such as user stories and sprints. More user journeys will be added over time as the project evolves. Some of the candidate topics include: Home Automation Turn traditional door locks into smart locks with Face Recognition. Alert parents if a crying toddler is left unattended for more than 15 minutes. Raise an alert if a baby is seated near a car door without child lock enabled while in motion. Business Automation Prevent accidents by alerting drivers who act sleepy or distracted. Make sure that a factory floor position is not left unattended for more than 15 minutes. Recognize presence of unauthorized people in a restricted access work area.","title":"User Journey"},{"location":"#user-system-interactions","text":"Users interact with the system in two phases: First Time Installation Consequent App Engagements The following diagram illustrates the high level user - system interactions.","title":"User - System Interactions"},{"location":"#user-interface-flow","text":"The User Interface is centered around three main activities: Setup Ambianic Edge to communicate with smart home devices: sensors, cameras, microphones, lights, door locks, and others. Design flows to automatically observe sensors and make helpful recommendations. Review event timeline, alerts and actionable recommendations. Ambianic UI is an Offline-First PWA (Progressive Web Application). PWAs work in any browser, but \"app-like\" with features such as being independent of connectivity, install to home screen, and push messaging depend on browser support. Ambianic UI does not assume that the user has constant broadband internet access. Its built to handle a range of real world scenarios with low bandwidth or no-Internet access at all when the user may need to review Ambianic alerts, events timeline, edit flows and configure edge devices. Ambianic UI stores data locally on the client device (mobile or desktop) and, when there\u2019s a network connection, syncs data to the user's Ambianic server and resolves any data conflicts. When possible it communicates directly with local Ambianic Edge devices minimizing network routing overhead.","title":"User Interface Flow"},{"location":"#project-status","text":"At this time, Ambianic is in active early formation stages. Design and implementation decisions are made daily with focus on advancing the project to an initial stable version as soon as possible. If you are willing to take the risk that comes with early stage code and are able to dive deep into Python, Javascript, Gstreamer, and Tensorflow code, then please keep reading. Otherwise you can register to watch for new releases . We will notify you as soon as a stable release is out.","title":"Project Status"},{"location":"#product-design-goals","text":"Our goal is to build a product that is useful out of the box: Less than 15 minutes setup time Less than $75 in hardware costs Primary platform: Raspberry Pi 4 B, 4GB RAM, 32GB SDRAM No coding required to get started Decomposable and hackable for open source developers","title":"Product Design Goals"},{"location":"#quick-start","text":"If you would like to try the latest version, follow the steps in the Quick Start Guide .","title":"Quick Start"},{"location":"#contributors","text":"If you are interested in becoming a contributor to the project, please read the Contributing page and follow the steps. Looking forward to hearing from you!","title":"Contributors"},{"location":"developers/api-overview/","text":"API Overview REST Backend Python Frontend JS Other","title":"API Overview"},{"location":"developers/api-overview/#api-overview","text":"REST Backend Python Frontend JS Other","title":"API Overview"},{"location":"developers/api/","text":"Ambianic APIs API Overview More about our API architecture REST API More about REST API WebSockets API More about our WebSockets API MQTT API More about our MQTT API Backend Python API More about our Python API Frontend JavaScript API More about our JavaScript API","title":"APIs"},{"location":"developers/api/#ambianic-apis","text":"","title":"Ambianic APIs"},{"location":"developers/api/#api-overview","text":"More about our API architecture","title":"API Overview"},{"location":"developers/api/#rest-api","text":"More about REST API","title":"REST API"},{"location":"developers/api/#websockets-api","text":"More about our WebSockets API","title":"WebSockets API"},{"location":"developers/api/#mqtt-api","text":"More about our MQTT API","title":"MQTT API"},{"location":"developers/api/#backend-python-api","text":"More about our Python API","title":"Backend Python API"},{"location":"developers/api/#frontend-javascript-api","text":"More about our JavaScript API","title":"Frontend JavaScript API"},{"location":"developers/architecture/","text":"Ambianic High Level Architecture Pipelines Pipe Elements Sources Outputs Connecting Pipelines Integrations Ambianic's main goal is to observe the environment and provide helpful suggestions in the context of home and business automation. The user has complete ownership and control of Ambianic's input sensors, AI inference flows and processed data storage. All logic executes on user's own edge devices and data is stored only on these devices with optional backup on a user designated server. The main unit of work is the Ambianic pipeline which executes in the runtime of Ambianic Edge devices. The following diagram illustrates an example pipeline which takes as input a video stream URI source such as a surveillance camera and outputs object detections to a local directory. st=>start: Video Source op_obj=>operation: Object Detection AI op_sav1=>parallel: Storage Element io1=>inputoutput: save object detections to file op_face=>operation: Face Detection AI op_sav2=>parallel: Storage Element io2=>inputoutput: save face detections to file e=>end: Output to other pipelines st->op_obj op_obj(bottom)->op_sav1 op_sav1(path1, bottom)->op_face op_sav1(path2, right)->io1 op_face(bottom)->op_sav2 op_sav2(path1, bottom)->e op_sav2(path2, right)->io2 $(\".diagram\").flowchart(); Configuration Here is the corresponding configuration section for the pipeline above (normally located in config.yaml ): pipelines: daytime_front_door_watch: - source: rtsp://user:pass@192.168.86.111:554/Streaming/Channels/101 - detect_objects: # run ai inference on the input data model: tflite: ai_models/mobilenet_ssd_v2_coco_quant_postprocess.tflite edgetpu: ai_models/mobilenet_ssd_v2_coco_quant_postprocess_edgetpu.tflite labels: ai_models/coco_labels.txt confidence_threshold: 0.8 - save_detections: # save samples from the inference results output_directory: ./data/detections/front-door/objects positive_interval: 2 # how often (in seconds) to save samples with ANY results above the confidence threshold idle_interval: 6000 # how often (in seconds) to save samples with NO results above the confidence threshold - detect_faces: # run ai inference on the samples from the previous element output model: tflite: ai_models/mobilenet_ssd_v2_coco_quant_postprocess.tflite edgetpu: ai_models/mobilenet_ssd_v2_face_quant_postprocess_edgetpu.tflite labels: ai_models/coco_labels.txt confidence_threshold: 0.8 - save_detections: # save samples from the inference results output_directory: ./data/detections/front-door/faces positive_interval: 2 idle_interval: 600","title":"Architecture"},{"location":"developers/architecture/#ambianic-high-level-architecture","text":"Pipelines Pipe Elements Sources Outputs Connecting Pipelines Integrations Ambianic's main goal is to observe the environment and provide helpful suggestions in the context of home and business automation. The user has complete ownership and control of Ambianic's input sensors, AI inference flows and processed data storage. All logic executes on user's own edge devices and data is stored only on these devices with optional backup on a user designated server. The main unit of work is the Ambianic pipeline which executes in the runtime of Ambianic Edge devices. The following diagram illustrates an example pipeline which takes as input a video stream URI source such as a surveillance camera and outputs object detections to a local directory. st=>start: Video Source op_obj=>operation: Object Detection AI op_sav1=>parallel: Storage Element io1=>inputoutput: save object detections to file op_face=>operation: Face Detection AI op_sav2=>parallel: Storage Element io2=>inputoutput: save face detections to file e=>end: Output to other pipelines st->op_obj op_obj(bottom)->op_sav1 op_sav1(path1, bottom)->op_face op_sav1(path2, right)->io1 op_face(bottom)->op_sav2 op_sav2(path1, bottom)->e op_sav2(path2, right)->io2 $(\".diagram\").flowchart();","title":"Ambianic High Level Architecture"},{"location":"developers/architecture/#configuration","text":"Here is the corresponding configuration section for the pipeline above (normally located in config.yaml ): pipelines: daytime_front_door_watch: - source: rtsp://user:pass@192.168.86.111:554/Streaming/Channels/101 - detect_objects: # run ai inference on the input data model: tflite: ai_models/mobilenet_ssd_v2_coco_quant_postprocess.tflite edgetpu: ai_models/mobilenet_ssd_v2_coco_quant_postprocess_edgetpu.tflite labels: ai_models/coco_labels.txt confidence_threshold: 0.8 - save_detections: # save samples from the inference results output_directory: ./data/detections/front-door/objects positive_interval: 2 # how often (in seconds) to save samples with ANY results above the confidence threshold idle_interval: 6000 # how often (in seconds) to save samples with NO results above the confidence threshold - detect_faces: # run ai inference on the samples from the previous element output model: tflite: ai_models/mobilenet_ssd_v2_coco_quant_postprocess.tflite edgetpu: ai_models/mobilenet_ssd_v2_face_quant_postprocess_edgetpu.tflite labels: ai_models/coco_labels.txt confidence_threshold: 0.8 - save_detections: # save samples from the inference results output_directory: ./data/detections/front-door/faces positive_interval: 2 idle_interval: 600","title":"Configuration"},{"location":"developers/cloud-deployment/","text":"Cloud Deployment Scenarios Private Cloud Public Cloud","title":"Cloud Deployment Scenarios"},{"location":"developers/cloud-deployment/#cloud-deployment-scenarios","text":"Private Cloud Public Cloud","title":"Cloud Deployment Scenarios"},{"location":"developers/deployment/","text":"Deployment Scenarios Minimal deployment Reference Architecture Scaling deployments Scaling Up Scaling Out Reference Architecture","title":"Deployment Scenarios"},{"location":"developers/deployment/#deployment-scenarios","text":"Minimal deployment Reference Architecture Scaling deployments Scaling Up Scaling Out Reference Architecture","title":"Deployment Scenarios"},{"location":"developers/edge-deployment/","text":"Edge IoT Deployment Scenarios Raspberry Pi Minimal install, capacity planning, scaling, monitoring.","title":"Edge IoT Deployment Scenarios"},{"location":"developers/edge-deployment/#edge-iot-deployment-scenarios","text":"Raspberry Pi Minimal install, capacity planning, scaling, monitoring.","title":"Edge IoT Deployment Scenarios"},{"location":"developers/hass-integration/","text":"Home Assistant Integration TBD","title":"Home Assistant Integration"},{"location":"developers/hass-integration/#home-assistant-integration","text":"TBD","title":"Home Assistant Integration"},{"location":"developers/home-bridge-integration/","text":"Home Bridge Integration TBD","title":"Home Bridge Integration"},{"location":"developers/home-bridge-integration/#home-bridge-integration","text":"TBD","title":"Home Bridge Integration"},{"location":"developers/integration/","text":"Integration with Smart Home hubs Home Assistant More on Home Assistant Integration Home Bridge More on Home Bridge Integration","title":"Integrations"},{"location":"developers/integration/#integration-with-smart-home-hubs","text":"","title":"Integration with Smart Home hubs"},{"location":"developers/integration/#home-assistant","text":"More on Home Assistant Integration","title":"Home Assistant"},{"location":"developers/integration/#home-bridge","text":"More on Home Bridge Integration","title":"Home Bridge"},{"location":"developers/js-api/","text":"Frontend JavaScript API TBD","title":"Frontend JavaScript API"},{"location":"developers/js-api/#frontend-javascript-api","text":"TBD","title":"Frontend JavaScript API"},{"location":"developers/mqtt-api/","text":"MQTT API TBD","title":"MQTT API"},{"location":"developers/mqtt-api/#mqtt-api","text":"TBD","title":"MQTT API"},{"location":"developers/publish-bazaar/","text":"Publishing to AI Apps Bazaar TBD","title":"Publishing to AI Apps Bazaar"},{"location":"developers/publish-bazaar/#publishing-to-ai-apps-bazaar","text":"TBD","title":"Publishing to AI Apps Bazaar"},{"location":"developers/python-api/","text":"Backend Python API TBD High level description link to raw API","title":"Backend Python API"},{"location":"developers/python-api/#backend-python-api","text":"TBD High level description link to raw API","title":"Backend Python API"},{"location":"developers/rest-api/","text":"REST API TBD","title":"REST API"},{"location":"developers/rest-api/#rest-api","text":"TBD","title":"REST API"},{"location":"developers/websockets-api/","text":"WebSockets API TBD","title":"WebSockets API"},{"location":"developers/websockets-api/#websockets-api","text":"TBD","title":"WebSockets API"},{"location":"developers/writing-plugins-overview/","text":"How to Write and Publish an Ambianic Plugin Ambianic can be extended via custom plugins. Two types of plugins are supported: Edge plugins, and UI plugins Edge plugins execute in an Ambianic Edge device environment, whereas UI plugins execute in the Ambianic UI PWA environment. It is common for a plugin to be built as a package with an Edge and an UI plugin designed to work together for a smooth end-to-end user experience. Let's take for example a VideoSource plugin that reads data from video streams such as live security camera feeds and feeds image frames to an Ambianic AI flow for inference such as person detection. The plugin will have two parts: An edge device plugin VideoSourceEdge which will reside in an Ambianic Edge device and execute in the core Ambianic runtime. It will be installed and configured on the edge device via secure shell access, its public REST APIs or GUI exposed by the VideoSourceUI plugin. An UI plugin VideoSourceUI which will implement GUI widgets for users to configure and control VideoSource connections. This plugin will enrich the visual user experience with the Ambianic UI PWA. st=>start: VideoSource Plugin install=>parallel: Install Plugin edge_plugin=>operation: VideoSourceEdge edge_env=>operation: Ambianic Edge Device src_connect=>inputoutput: Process video streams edge_flow=>operation: AI flow ui_plugin=>operation: VideoSourceUI ui_env=>operation: Ambianic UI PWA user_config=>inputoutput: GUI Configuration st(bottom)->install install(path1, right)->edge_plugin edge_plugin(bottom)->edge_env edge_env(bottom)->src_connect src_connect(bottom)->edge_flow install(path2, bottom)->ui_plugin ui_plugin(bottom)->ui_env ui_env(bottom)->user_config $(\".plugin-overview-diagram\").flowchart(); Anatomy of an Ambianic Edge plugin Ambianic Edge plugins are written in Python. They are essentially pipeline elements. See the architecture document for an overview of pipelines and pipeline elements. An edge plugin must provide at a minimum: Input: Declare input data types Provide implementation of input data consumer Output: Declare output data types Provide implementation of output data producer Configuration: Declare configuration parameters Lifecycle methods: Start Stop Heal An edge plugin may also provide: REST API: Declare public REST APIs in OpenAPI format. Provide implementation for REST APIs Anatomy of an Ambianic UI plugin An UI plugin must provide at a minimum: Install/Uninstall visual UI flow Configuration visual UI flow Runtime data views: Event Timeline: inputs and outputs Alerts: Critical, Warning, Info Stats: such as performance and error rates Event curation: Prioritization: Alert Warning Info Avatar: Image Name Color Follow up Actions: Checkbox Button Link Text Speech Gesture Custom Data curation for plugins with AI inference: Infenrece label correction Adding new labels Removing labels Plugin bundles A plugin bundle consists of: Edge plugin UI plugin A basic edge plugin template The following abstract Python class implements the minimum sceleton of an edge plugin. Plugin implementations should extend this class and add custom logic as we will see further in this document with a more advanced example. class PipeElement(ManagedService): \"\"\"The basic building block of an Ambianic pipeline.\"\"\" def __init__(self): super().__init__() self._state = PIPE_STATE_STOPPED self._next_element = None self._latest_heartbeat = time.monotonic() @property def state(self): \"\"\"Lifecycle state of the pipe element.\"\"\" return self._state def start(self): \"\"\"Only sourcing elements (first in a pipeline) need to override. It is invoked once when the enclosing pipeline is started. It should continue to run until the corresponding stop() method is invoked on the same object from a separate pipeline lifecycle manager thread. It is recommended for overriding methods to invoke this base method via super().start() before proceeding with custom logic. \"\"\" self._state = PIPE_STATE_RUNNING @abc.abstractmethod def heal(self): # pragma: no cover \"\"\"Override with adequate implementation of a healing procedure. heal() is invoked by a lifecycle manager when its determined that the element does not respond within reasonable timeframe. This can happen for example if the element depends on external IO resources, which become unavailable for an extended period of time. The healing procedure should be considered a chance to recover or find an alternative way to proceed. If heal does not reset the pipe element back to a responsive state, it is likely that the lifecycle manager will stop the element and its ecnlosing pipeline. \"\"\" pass def healthcheck(self): \"\"\"Check the health of this element. :returns: (timestamp, status) tuple with most recent heartbeat timestamp and health status code ('OK' normally). \"\"\" status = 'OK' # At some point status may carry richer information return self._latest_heartbeat, status def heartbeat(self): \"\"\"Set the heartbeat timestamp to time.monotonic(). Keeping the heartbeat timestamp current informs the lifecycle manager that this element is functioning well. \"\"\" now = time.monotonic() self._latest_heartbeat = now def stop(self): \"\"\"Receive stop signal and act accordingly. Subclasses implementing sourcing elements should override this method by first invoking their super class implementation and then running through steps specific to stopping their ongoing sample processing. \"\"\" self._state = PIPE_STATE_STOPPED def connect_to_next_element(self, next_element=None): \"\"\"Connect this element to the next element in the pipe. Subclasses should not override this method. \"\"\" assert next_element assert isinstance(next_element, PipeElement) self._next_element = next_element def receive_next_sample(self, **sample): \"\"\"Receive next sample from a connected previous element if applicable. All pipeline elements except for the first (sourcing) element in the pipeline will depend on this method to feed them with new samples to process. Subclasses should not override this method. :Parameters: ---------- **sample : dict A dict of (key, value) pairs that represent the sample. It is left to specialized implementations of PipeElement to specify their in/out sample formats and enforce compatibility with adjacent connected pipe elements. \"\"\" self.heartbeat() for processed_sample in self.process_sample(**sample): if self._next_element: if (processed_sample): self._next_element.receive_next_sample(**processed_sample) else: self._next_element.receive_next_sample() self.heartbeat() @abc.abstractmethod # pragma: no cover def process_sample(self, **sample) -> Iterable[dict]: \"\"\"Override and implement as generator. Invoked by receive_next_sample() when the previous element (or pipeline source) feeds another data input sample. Implementing subclasses should process input samples and yield output samples for the next element in the pipeline. :Parameters: ---------- **sample : dict A dict of (key, value) pairs that represent the sample. It is left to specialized implementations of PipeElement to specify their in/out sample formats and enforce compatibility with adjacent connected pipe elements. :Returns: ---------- processed_sample: Iterable[dict] Generates processed samples to be passed on to the next pipeline element. \"\"\" yield sample The following sequence diagram illustrates an example flow of data from an external security camera source through VideoSourceEdge and the rest of an object detection pipeline. sequenceDiagram participant Cam as Security Camera participant Source as VideoSourceEdge participant Detect as ObjectDetection participant Store as StoreDetection participant File as File System loop Until pipeline.stop() Cam->>Source: rtsp://ip/video/channel0 Source->>Detect: process_sample(image) Detect->>Store: process_sample(det_boxes) Store->>File: file.write(image, det_json) end A basic UI plugin template A basic plugin bundle template A more advanced plugin example Required components of a plugin Optional components of a plugin Writing a custom plugins: step by step Testing plugins Packaging plugins Packaging and Edge plugin Use a standard python wheel and publish on Pypi Packaging and UI plugin Use a standard npm package format and publish on npm Packaging a bundle of plugins In the UI plugin, provide reference to the edge plugin package: PyPi wheel name package version Documenting plugins Publishing plugins More about Publishing your AI app","title":"Writing PLugins"},{"location":"developers/writing-plugins-overview/#how-to-write-and-publish-an-ambianic-plugin","text":"Ambianic can be extended via custom plugins. Two types of plugins are supported: Edge plugins, and UI plugins Edge plugins execute in an Ambianic Edge device environment, whereas UI plugins execute in the Ambianic UI PWA environment. It is common for a plugin to be built as a package with an Edge and an UI plugin designed to work together for a smooth end-to-end user experience. Let's take for example a VideoSource plugin that reads data from video streams such as live security camera feeds and feeds image frames to an Ambianic AI flow for inference such as person detection. The plugin will have two parts: An edge device plugin VideoSourceEdge which will reside in an Ambianic Edge device and execute in the core Ambianic runtime. It will be installed and configured on the edge device via secure shell access, its public REST APIs or GUI exposed by the VideoSourceUI plugin. An UI plugin VideoSourceUI which will implement GUI widgets for users to configure and control VideoSource connections. This plugin will enrich the visual user experience with the Ambianic UI PWA. st=>start: VideoSource Plugin install=>parallel: Install Plugin edge_plugin=>operation: VideoSourceEdge edge_env=>operation: Ambianic Edge Device src_connect=>inputoutput: Process video streams edge_flow=>operation: AI flow ui_plugin=>operation: VideoSourceUI ui_env=>operation: Ambianic UI PWA user_config=>inputoutput: GUI Configuration st(bottom)->install install(path1, right)->edge_plugin edge_plugin(bottom)->edge_env edge_env(bottom)->src_connect src_connect(bottom)->edge_flow install(path2, bottom)->ui_plugin ui_plugin(bottom)->ui_env ui_env(bottom)->user_config $(\".plugin-overview-diagram\").flowchart();","title":"How to Write and Publish an Ambianic Plugin"},{"location":"developers/writing-plugins-overview/#anatomy-of-an-ambianic-edge-plugin","text":"Ambianic Edge plugins are written in Python. They are essentially pipeline elements. See the architecture document for an overview of pipelines and pipeline elements. An edge plugin must provide at a minimum: Input: Declare input data types Provide implementation of input data consumer Output: Declare output data types Provide implementation of output data producer Configuration: Declare configuration parameters Lifecycle methods: Start Stop Heal An edge plugin may also provide: REST API: Declare public REST APIs in OpenAPI format. Provide implementation for REST APIs","title":"Anatomy of an Ambianic Edge plugin"},{"location":"developers/writing-plugins-overview/#anatomy-of-an-ambianic-ui-plugin","text":"An UI plugin must provide at a minimum: Install/Uninstall visual UI flow Configuration visual UI flow Runtime data views: Event Timeline: inputs and outputs Alerts: Critical, Warning, Info Stats: such as performance and error rates Event curation: Prioritization: Alert Warning Info Avatar: Image Name Color Follow up Actions: Checkbox Button Link Text Speech Gesture Custom Data curation for plugins with AI inference: Infenrece label correction Adding new labels Removing labels","title":"Anatomy of an Ambianic UI plugin"},{"location":"developers/writing-plugins-overview/#plugin-bundles","text":"A plugin bundle consists of: Edge plugin UI plugin","title":"Plugin bundles"},{"location":"developers/writing-plugins-overview/#a-basic-edge-plugin-template","text":"The following abstract Python class implements the minimum sceleton of an edge plugin. Plugin implementations should extend this class and add custom logic as we will see further in this document with a more advanced example. class PipeElement(ManagedService): \"\"\"The basic building block of an Ambianic pipeline.\"\"\" def __init__(self): super().__init__() self._state = PIPE_STATE_STOPPED self._next_element = None self._latest_heartbeat = time.monotonic() @property def state(self): \"\"\"Lifecycle state of the pipe element.\"\"\" return self._state def start(self): \"\"\"Only sourcing elements (first in a pipeline) need to override. It is invoked once when the enclosing pipeline is started. It should continue to run until the corresponding stop() method is invoked on the same object from a separate pipeline lifecycle manager thread. It is recommended for overriding methods to invoke this base method via super().start() before proceeding with custom logic. \"\"\" self._state = PIPE_STATE_RUNNING @abc.abstractmethod def heal(self): # pragma: no cover \"\"\"Override with adequate implementation of a healing procedure. heal() is invoked by a lifecycle manager when its determined that the element does not respond within reasonable timeframe. This can happen for example if the element depends on external IO resources, which become unavailable for an extended period of time. The healing procedure should be considered a chance to recover or find an alternative way to proceed. If heal does not reset the pipe element back to a responsive state, it is likely that the lifecycle manager will stop the element and its ecnlosing pipeline. \"\"\" pass def healthcheck(self): \"\"\"Check the health of this element. :returns: (timestamp, status) tuple with most recent heartbeat timestamp and health status code ('OK' normally). \"\"\" status = 'OK' # At some point status may carry richer information return self._latest_heartbeat, status def heartbeat(self): \"\"\"Set the heartbeat timestamp to time.monotonic(). Keeping the heartbeat timestamp current informs the lifecycle manager that this element is functioning well. \"\"\" now = time.monotonic() self._latest_heartbeat = now def stop(self): \"\"\"Receive stop signal and act accordingly. Subclasses implementing sourcing elements should override this method by first invoking their super class implementation and then running through steps specific to stopping their ongoing sample processing. \"\"\" self._state = PIPE_STATE_STOPPED def connect_to_next_element(self, next_element=None): \"\"\"Connect this element to the next element in the pipe. Subclasses should not override this method. \"\"\" assert next_element assert isinstance(next_element, PipeElement) self._next_element = next_element def receive_next_sample(self, **sample): \"\"\"Receive next sample from a connected previous element if applicable. All pipeline elements except for the first (sourcing) element in the pipeline will depend on this method to feed them with new samples to process. Subclasses should not override this method. :Parameters: ---------- **sample : dict A dict of (key, value) pairs that represent the sample. It is left to specialized implementations of PipeElement to specify their in/out sample formats and enforce compatibility with adjacent connected pipe elements. \"\"\" self.heartbeat() for processed_sample in self.process_sample(**sample): if self._next_element: if (processed_sample): self._next_element.receive_next_sample(**processed_sample) else: self._next_element.receive_next_sample() self.heartbeat() @abc.abstractmethod # pragma: no cover def process_sample(self, **sample) -> Iterable[dict]: \"\"\"Override and implement as generator. Invoked by receive_next_sample() when the previous element (or pipeline source) feeds another data input sample. Implementing subclasses should process input samples and yield output samples for the next element in the pipeline. :Parameters: ---------- **sample : dict A dict of (key, value) pairs that represent the sample. It is left to specialized implementations of PipeElement to specify their in/out sample formats and enforce compatibility with adjacent connected pipe elements. :Returns: ---------- processed_sample: Iterable[dict] Generates processed samples to be passed on to the next pipeline element. \"\"\" yield sample The following sequence diagram illustrates an example flow of data from an external security camera source through VideoSourceEdge and the rest of an object detection pipeline. sequenceDiagram participant Cam as Security Camera participant Source as VideoSourceEdge participant Detect as ObjectDetection participant Store as StoreDetection participant File as File System loop Until pipeline.stop() Cam->>Source: rtsp://ip/video/channel0 Source->>Detect: process_sample(image) Detect->>Store: process_sample(det_boxes) Store->>File: file.write(image, det_json) end","title":"A basic edge plugin template"},{"location":"developers/writing-plugins-overview/#a-basic-ui-plugin-template","text":"","title":"A basic UI plugin template"},{"location":"developers/writing-plugins-overview/#a-basic-plugin-bundle-template","text":"","title":"A basic plugin bundle template"},{"location":"developers/writing-plugins-overview/#a-more-advanced-plugin-example","text":"","title":"A more advanced plugin example"},{"location":"developers/writing-plugins-overview/#required-components-of-a-plugin","text":"","title":"Required components of a plugin"},{"location":"developers/writing-plugins-overview/#optional-components-of-a-plugin","text":"","title":"Optional components of a plugin"},{"location":"developers/writing-plugins-overview/#writing-a-custom-plugins-step-by-step","text":"","title":"Writing a custom plugins: step by step"},{"location":"developers/writing-plugins-overview/#testing-plugins","text":"","title":"Testing plugins"},{"location":"developers/writing-plugins-overview/#packaging-plugins","text":"","title":"Packaging plugins"},{"location":"developers/writing-plugins-overview/#packaging-and-edge-plugin","text":"Use a standard python wheel and publish on Pypi","title":"Packaging and Edge plugin"},{"location":"developers/writing-plugins-overview/#packaging-and-ui-plugin","text":"Use a standard npm package format and publish on npm","title":"Packaging and UI plugin"},{"location":"developers/writing-plugins-overview/#packaging-a-bundle-of-plugins","text":"In the UI plugin, provide reference to the edge plugin package: PyPi wheel name package version","title":"Packaging a bundle of plugins"},{"location":"developers/writing-plugins-overview/#documenting-plugins","text":"","title":"Documenting plugins"},{"location":"developers/writing-plugins-overview/#publishing-plugins","text":"More about Publishing your AI app","title":"Publishing plugins"},{"location":"legal/CONTRIBUTING/","text":"How to Contribute We'd love to accept your patches and contributions to this project. There are just a few small guidelines you need to follow. Contributor License Agreement Contributions to this project must be accompanied by a Contributor License Agreement. You (or your employer) retain the copyright to your contribution; this simply gives us permission to use and redistribute your contributions as part of the project. Head over to https://ambianic.ai/cla to see your current agreements on file or to sign a new one. You generally only need to submit a CLA once, so if you've already submitted one (even if it was for a different project), you probably don't need to do it again. Code reviews All submissions, including submissions by project members, require review. We use GitHub pull requests for this purpose. Consult GitHub Help for more information on using pull requests. Community Guidelines This project follows Google's Open Source Community Guidelines .","title":"Contributing"},{"location":"legal/CONTRIBUTING/#how-to-contribute","text":"We'd love to accept your patches and contributions to this project. There are just a few small guidelines you need to follow.","title":"How to Contribute"},{"location":"legal/CONTRIBUTING/#contributor-license-agreement","text":"Contributions to this project must be accompanied by a Contributor License Agreement. You (or your employer) retain the copyright to your contribution; this simply gives us permission to use and redistribute your contributions as part of the project. Head over to https://ambianic.ai/cla to see your current agreements on file or to sign a new one. You generally only need to submit a CLA once, so if you've already submitted one (even if it was for a different project), you probably don't need to do it again.","title":"Contributor License Agreement"},{"location":"legal/CONTRIBUTING/#code-reviews","text":"All submissions, including submissions by project members, require review. We use GitHub pull requests for this purpose. Consult GitHub Help for more information on using pull requests.","title":"Code reviews"},{"location":"legal/CONTRIBUTING/#community-guidelines","text":"This project follows Google's Open Source Community Guidelines .","title":"Community Guidelines"},{"location":"users/ai-apps-bazaar/","text":"About the AI Apps Bazaar Github Repo URL Docker hub URL?","title":"About the AI Apps Bazaar"},{"location":"users/ai-apps-bazaar/#about-the-ai-apps-bazaar","text":"Github Repo URL Docker hub URL?","title":"About the AI Apps Bazaar"},{"location":"users/ai-apps/","text":"Ambianic AI Apps Standard apps Custom apps Apps Bazaar","title":"AI Apps"},{"location":"users/ai-apps/#ambianic-ai-apps","text":"Standard apps Custom apps Apps Bazaar","title":"Ambianic AI Apps"},{"location":"users/configure/","text":"Configuring Ambianic Quickstart reference Most common configuration settings Advanced configuration Configuration changes: runtime safe and restart changes st=>start: Start|past:>http://www.google.com[blank] e=>end: Ende|future:>http://www.google.com op1=>operation: My Operation|past op2=>operation: Stuff|current sub1=>subroutine: My Subroutine|invalid cond=>condition: Yes or No?|approved:>http://www.google.com c2=>condition: Good idea|rejected io=>inputoutput: catch something...|future st->op1(right)->cond cond(yes, right)->c2 cond(no)->sub1(left)->op1 c2(yes)->io->e c2(no)->op2->e $(\".diagram\").flowchart();","title":"Configuring"},{"location":"users/configure/#configuring-ambianic","text":"Quickstart reference Most common configuration settings Advanced configuration Configuration changes: runtime safe and restart changes st=>start: Start|past:>http://www.google.com[blank] e=>end: Ende|future:>http://www.google.com op1=>operation: My Operation|past op2=>operation: Stuff|current sub1=>subroutine: My Subroutine|invalid cond=>condition: Yes or No?|approved:>http://www.google.com c2=>condition: Good idea|rejected io=>inputoutput: catch something...|future st->op1(right)->cond cond(yes, right)->c2 cond(no)->sub1(left)->op1 c2(yes)->io->e c2(no)->op2->e $(\".diagram\").flowchart();","title":"Configuring Ambianic"},{"location":"users/custom-ai-apps/","text":"Installing and Running Custom AI Apps Examples of custom apps Where to get custom apps How to install and run","title":"Installing and Running Custom AI Apps"},{"location":"users/custom-ai-apps/#installing-and-running-custom-ai-apps","text":"Examples of custom apps Where to get custom apps How to install and run","title":"Installing and Running Custom AI Apps"},{"location":"users/faq/","text":"Frequently Asked Questions Does Ambianic use hardware accelerators for AI inference? Yes. Ambianic currently supports the Google Coral EdgeTPU. The server relies on the Tensorflow Lite Runtime to dynamically detect Coral. If no EdgeTPU is available, AI inference falls back on the CPU. Check your logs for messages indicating EdgeTPU availability. Does Ambianic use hardware accelerators for video processing? Yes. Ambianic relies on gstreamer to dynamically detect and use any available GPU on the host platform for video and image processing. Asked Asked Asked 6. 2. TBD","title":"FAQ"},{"location":"users/faq/#frequently-asked-questions","text":"Does Ambianic use hardware accelerators for AI inference? Yes. Ambianic currently supports the Google Coral EdgeTPU. The server relies on the Tensorflow Lite Runtime to dynamically detect Coral. If no EdgeTPU is available, AI inference falls back on the CPU. Check your logs for messages indicating EdgeTPU availability. Does Ambianic use hardware accelerators for video processing? Yes. Ambianic relies on gstreamer to dynamically detect and use any available GPU on the host platform for video and image processing. Asked Asked Asked 6. 2. TBD","title":"Frequently Asked Questions"},{"location":"users/howto/","text":"How To TBD","title":"How To"},{"location":"users/howto/#how-to","text":"TBD","title":"How To"},{"location":"users/install/","text":"Installing and Running Ambianic Quickstart reference Various install scenarios: CPU, OS matrix","title":"Installing"},{"location":"users/install/#installing-and-running-ambianic","text":"Quickstart reference Various install scenarios: CPU, OS matrix","title":"Installing and Running Ambianic"},{"location":"users/mobileui/","text":"Amianic Mobile UI Registration and login Your Inference Timeline Alerts and Notifications","title":"Mobile UI"},{"location":"users/mobileui/#amianic-mobile-ui","text":"Registration and login Your Inference Timeline Alerts and Notifications","title":"Amianic Mobile UI"},{"location":"users/quickstart/","text":"Quick Start Ambianic's main goal is to provide helpful suggestions in the context of home and business automation. The main unit of work is the Ambianic pipeline. The following diagram illustrates an example pipeline that takes as input a video stream URI source such as a surveillance camera and outputs object detections to a local directory. st=>start: Video Source op_obj=>operation: Object Detection AI op_sav1=>parallel: Storage Element io1=>inputoutput: save object detections to file op_face=>operation: Face Detection AI op_sav2=>parallel: Storage Element io2=>inputoutput: save face detections to file e=>end: Output to other pipelines st->op_obj op_obj(bottom)->op_sav1 op_sav1(path1, bottom)->op_face op_sav1(path2, right)->io1 op_face(bottom)->op_sav2 op_sav2(path1, bottom)->e op_sav2(path2, right)->io2 $(\".diagram\").flowchart(); Ambianic Deployment Ambianic has two major deployment components: Ambianic Edge and Ambianic UI. Ambanic Edge Deployment The easiest way to deploy Ambianic Edge is via its Docker distribution . The recommended deployment platform for Ambianic Edge is Raspberry Pi 4 with 4GB RAM and 32GB SD card . The docker image is availbale for ARM and x86 architectures so it can be deployed on most modern machines and operating systems. To deploy on a Raspberry Pi 4, you will need a recent Raspbian install with Docker on it . You can install and run the image in the default pi user space on Raspbian. Configuration Before starting the image, you will need to designate a workspace directory with an initial config.yaml file in it. For example, the directory could be named /opt/ambianid-edge.workspace . Here is an example config.yaml file ###################################### # Ambianic main configuration file # ###################################### version: '1.0.10' # path to the data directory data_dir: &data_dir ./data # Set logging level to one of DEBUG, INFO, WARNING, ERROR logging: file: ./data/ambianic-log.txt level: DEBUG # Pipeline event timeline configuration timeline: event_log: ./data/timeline-event-log.yaml # Cameras and other input data sources # Using Home Assistant conventions to ease upcoming integration sources: front_door_camera: &src_front_door_cam uri: *secret_uri_front_door_camera # type: video, audio, or auto type: video live: true entry_area_camera: &src_entry_area_cam uri: *secret_uri_entry_area_camera type: video # live: is this a live source or a recording # when live is True, the AVSource element will try to reconnect # if the stream is interrupted due to network disruption or another reason. live: true # recorded front door cam feed for quick testing # recorded_cam_feed: &src_recorded_cam_feed # uri: file:///workspace/tests/pipeline/avsource/test2-cam-person1.mkv # type: video ai_models: # image_classification: # tf_graph: # labels: image_detection: &tfm_image_detection model: tflite: /opt/ambianic-edge/ai_models/mobilenet_ssd_v2_coco_quant_postprocess.tflite edgetpu: /opt/ambianic-edge/ai_models/mobilenet_ssd_v2_coco_quant_postprocess_edgetpu.tflite labels: /opt/ambianic-edge/ai_models/coco_labels.txt face_detection: &tfm_face_detection model: tflite: /opt/ambianic-edge/ai_models/mobilenet_ssd_v2_coco_quant_postprocess.tflite edgetpu: /opt/ambianic-edge/ai_models/mobilenet_ssd_v2_face_quant_postprocess_edgetpu.tflite labels: /opt/ambianic-edge/ai_models/coco_labels.txt top_k: 2 # A named pipeline defines an ordered sequence of operations # such as reading from a data source, AI model inference, saving samples and others. pipelines: # sequence of piped operations for use in daytime front door watch front_door_watch: - source: *src_front_door_cam - detect_objects: # run ai inference on the input data <<: *tfm_image_detection confidence_threshold: 0.8 - save_detections: # save samples from the inference results positive_interval: 2 # how often (in seconds) to save samples with ANY results above the confidence threshold idle_interval: 6000 # how often (in seconds) to save samples with NO results above the confidence threshold - detect_faces: # run ai inference on the samples from the previous element output <<: *tfm_face_detection confidence_threshold: 0.8 - save_detections: # save samples from the inference results positive_interval: 2 idle_interval: 600 # sequence of piped operations for use in daytime front door watch entry_area_watch: - source: *src_entry_area_cam # - mask: svg # apply a mask to the input data - detect_objects: # run ai inference on the input data <<: *tfm_image_detection confidence_threshold: 0.8 - save_detections: # save samples from the inference results positive_interval: 2 # how often (in seconds) to save samples with ANY results above the confidence thresho$ idle_interval: 6000 # how often (in seconds) to save samples with NO results above the confidence threshold - detect_faces: # run ai inference on the samples from the previous element output <<: *tfm_face_detection confidence_threshold: 0.8 - save_detections: # save samples from the inference results positive_interval: 2 idle_interval: 600 In the configuration excerpt above, there are a few references to variables defined elsewhere in the YAML file. *src_front_door_cam and *src_entry_area_cam are the only references that you have to understand and configure in order to get Ambianic Edge working with your own cameras (or other source of video feed). Here is the definition of this variable reference that you will find in config.yaml: Since camera URIs often contain credentials, we recommended that you store these values in secrets.yaml which needs to be located in the same directory as config.yaml . Ambianid Edge will automatically look for and if available prepend secrets.yaml to config.yaml . The idea here is that you can share config.yaml with others without compromising privacy sensitive parameters. An example valid entry in secretes.yaml for a camera URI, would look like this: secret_uri_front_door_camera: &secret_uri_front_door_camera 'rtsp://user:pass@192.168.86.111:554/Streaming/Channels/101' # add more secret entries as regular yaml mappings The rest of the configuration settings can be left with their default values for now. Notice that each pipeline definition starts with a media source. You can add and remove pipelines to match your home setup. Once you specify the URI of your camera(s), you can start the Docker image. Starting Ambianic Edge Here is an example startup line for Ambianic Edge. We will go over each parameter below: # test if coral usb stick is available USB_DIR=/dev/bus/usb if [ -d \"$USB_DIR\" ]; then USB_ARG=\"--device $USB_DIR\" else USB_ARG=\"\" fi # check if there is an image update docker pull ambianic/ambianic-edge:latest # run latest image docker run -it --rm \\ --name ambianic-edge \\ --mount type=bind,source=\"/opt/workspace\",target=/workspace \\ --publish 8778:8778 \\ $USB_ARG \\ ambianic/ambianic:latest Let's review the start script above line by line: # test if coral usb stick is available USB_DIR=/dev/bus/usb if [ -d \"$USB_DIR\" ]; then USB_ARG=\"--device $USB_DIR\" else USB_ARG=\"\" fi Ambianic Edge is able to automatically detect and use the Google Coral EdgeTPU for AI inference. Coral is a powerful USB stick that can speed up inference 5-10 times. However AI inference is only part of all the functions that execute in an Ambianic Edge pipeline. Video decoding and formatting also takes substantial amount of processing time. Overall we've seen Coral to improve performance 30-50% on a realistic Raspberry Pi 4 deployment with multiple simultaneous pipelines pulling images from multiple cameras and processing these images with multiple inference models. If you don't have a Coral device available, no need to worry for now. Raspberry Pi 4 is powerful enough to handle multiple simultaneous camera sourced pipelines with approximately 1-2 frames per second (fps). An objects or person of interest would normally show up in one of your cameras for at leastd one second, which is enough time to be registered and processed by Ambianic Edge on a plain RPI4. Let's look at the other docker image start parameters: docker pull ambianic/ambianic-edge:latest This line ensures that we fetch the latest available Ambianic Edge image. There are weekly semanctic releases with fixes and new updates. Generally this should work for most deployments. Occasionally there may be new releases with breaking changes. Its important to keep an eye on the release notes and make any configuration adjustments if necessary. --name ambianic-edge \\ This parameter just gives a local friendly name of the container that will show up in a docker container listing instead of a cryptic hash tag. --mount type=bind,source=\"/opt/workspace\",target=/workspace \\ This parameter tells Docker to bind your prepared workspace with config.yaml to the image internal /workspace directory. In addition to finding config.yaml , the image will use the workspace directory to write log and data files such as images with object detections. $USB_ARG \\ This is the dynamic parameter which is empty if Coral is not found on the USB bus. Otherwise it tells Docker to let Ambianic Edge access Coral on the usb bus. --publish 8778:8778 \\ This parameter exposes port 8778 where the Ambianic Edge REST API is available. ambianic/ambianic:latest The final parameter points to the latest Ambianic Edge image that Docker will execute. If you use Docker Compose for a more convenient management of multiple docker images, here is a configuration section that you can place in your docker-compose.yaml ambianic: container_name: ambianic restart: unless-stopped privileged: true image: ambianic/ambianic-edge:latest volumes: # - /dev/bus/usb:/dev/bus/usb - /opt/ambianic-edge.prod:/workspace ports: - 8778:8778 restart: on-failure healthcheck: test: [\"CMD\", \"curl\", \"-sI\", \"http://127.0.0.1:8778/\"] interval: 300s timeout: 1s retries: 10 Notice the line with the usb parameter. It would be commented out if there is no Coral attached to your RPI. Ambanic UI Deployment Ambianic UI is a prorgressive web application (PWA) that provides you with a friendly access to Ambianic Edge device data. For example you can see a timeline view with notable events around your home organized chronologically or by importance. Below is an example timeline screenshot. Public Ambanic UI app You can access the public app at https://ui.ambianic.ai/ Due to PWA security requirements it is currently not possible to connect directly from the public app to your local Ambianic Edge device. One solution is to setup a secure connection to your Ambinic Edge device via Dynamic DNS and Open SSH tunnel. However this is an involved process that could take a few hours of your time. There is an alternative solution via WebRTC that only takes a few moments to setup. We are working on it for the next Ambianic UI release. In the meanwhile, you can install Ambianic UI on a local machine - either the same one where Ambianic Edge runs or any other machine that runs on the same LAN. Here is how: Local deployment of Ambianic UI Ambianic UI is distributed as a Node JS npm package. https://www.npmjs.com/package/ambianic-ui If you are familiar with Node JS, you can install and run ambianic-ui from its npm distribution. Otherwise, you can follow these steps: Clone source repository git clone https://github.com/ambianic/ambianic-ui.git cd ambianic-ui Install dependencies npm install Compiles and hot-reloads for development npm run serve After a few moments you should see a message in your terminal window similar to this: App running at: - Local: http://localhost:8080/ - Network: http://192.168.86.246:8080/ Now you can access the app from your browser. The home screen looks like this: Go to the Settings page to point the UI to the host name of your Ambianic Edge device. You can test the connection to be sure that all is fine. Then navigate to the Timeline page to see detections from your cameras. If you experience problems with your initial setup, feel free to open an issue on github.","title":"Quick Start"},{"location":"users/quickstart/#quick-start","text":"Ambianic's main goal is to provide helpful suggestions in the context of home and business automation. The main unit of work is the Ambianic pipeline. The following diagram illustrates an example pipeline that takes as input a video stream URI source such as a surveillance camera and outputs object detections to a local directory. st=>start: Video Source op_obj=>operation: Object Detection AI op_sav1=>parallel: Storage Element io1=>inputoutput: save object detections to file op_face=>operation: Face Detection AI op_sav2=>parallel: Storage Element io2=>inputoutput: save face detections to file e=>end: Output to other pipelines st->op_obj op_obj(bottom)->op_sav1 op_sav1(path1, bottom)->op_face op_sav1(path2, right)->io1 op_face(bottom)->op_sav2 op_sav2(path1, bottom)->e op_sav2(path2, right)->io2 $(\".diagram\").flowchart();","title":"Quick Start"},{"location":"users/quickstart/#ambianic-deployment","text":"Ambianic has two major deployment components: Ambianic Edge and Ambianic UI.","title":"Ambianic Deployment"},{"location":"users/quickstart/#ambanic-edge-deployment","text":"The easiest way to deploy Ambianic Edge is via its Docker distribution . The recommended deployment platform for Ambianic Edge is Raspberry Pi 4 with 4GB RAM and 32GB SD card . The docker image is availbale for ARM and x86 architectures so it can be deployed on most modern machines and operating systems. To deploy on a Raspberry Pi 4, you will need a recent Raspbian install with Docker on it . You can install and run the image in the default pi user space on Raspbian.","title":"Ambanic Edge Deployment"},{"location":"users/quickstart/#configuration","text":"Before starting the image, you will need to designate a workspace directory with an initial config.yaml file in it. For example, the directory could be named /opt/ambianid-edge.workspace . Here is an example config.yaml file ###################################### # Ambianic main configuration file # ###################################### version: '1.0.10' # path to the data directory data_dir: &data_dir ./data # Set logging level to one of DEBUG, INFO, WARNING, ERROR logging: file: ./data/ambianic-log.txt level: DEBUG # Pipeline event timeline configuration timeline: event_log: ./data/timeline-event-log.yaml # Cameras and other input data sources # Using Home Assistant conventions to ease upcoming integration sources: front_door_camera: &src_front_door_cam uri: *secret_uri_front_door_camera # type: video, audio, or auto type: video live: true entry_area_camera: &src_entry_area_cam uri: *secret_uri_entry_area_camera type: video # live: is this a live source or a recording # when live is True, the AVSource element will try to reconnect # if the stream is interrupted due to network disruption or another reason. live: true # recorded front door cam feed for quick testing # recorded_cam_feed: &src_recorded_cam_feed # uri: file:///workspace/tests/pipeline/avsource/test2-cam-person1.mkv # type: video ai_models: # image_classification: # tf_graph: # labels: image_detection: &tfm_image_detection model: tflite: /opt/ambianic-edge/ai_models/mobilenet_ssd_v2_coco_quant_postprocess.tflite edgetpu: /opt/ambianic-edge/ai_models/mobilenet_ssd_v2_coco_quant_postprocess_edgetpu.tflite labels: /opt/ambianic-edge/ai_models/coco_labels.txt face_detection: &tfm_face_detection model: tflite: /opt/ambianic-edge/ai_models/mobilenet_ssd_v2_coco_quant_postprocess.tflite edgetpu: /opt/ambianic-edge/ai_models/mobilenet_ssd_v2_face_quant_postprocess_edgetpu.tflite labels: /opt/ambianic-edge/ai_models/coco_labels.txt top_k: 2 # A named pipeline defines an ordered sequence of operations # such as reading from a data source, AI model inference, saving samples and others. pipelines: # sequence of piped operations for use in daytime front door watch front_door_watch: - source: *src_front_door_cam - detect_objects: # run ai inference on the input data <<: *tfm_image_detection confidence_threshold: 0.8 - save_detections: # save samples from the inference results positive_interval: 2 # how often (in seconds) to save samples with ANY results above the confidence threshold idle_interval: 6000 # how often (in seconds) to save samples with NO results above the confidence threshold - detect_faces: # run ai inference on the samples from the previous element output <<: *tfm_face_detection confidence_threshold: 0.8 - save_detections: # save samples from the inference results positive_interval: 2 idle_interval: 600 # sequence of piped operations for use in daytime front door watch entry_area_watch: - source: *src_entry_area_cam # - mask: svg # apply a mask to the input data - detect_objects: # run ai inference on the input data <<: *tfm_image_detection confidence_threshold: 0.8 - save_detections: # save samples from the inference results positive_interval: 2 # how often (in seconds) to save samples with ANY results above the confidence thresho$ idle_interval: 6000 # how often (in seconds) to save samples with NO results above the confidence threshold - detect_faces: # run ai inference on the samples from the previous element output <<: *tfm_face_detection confidence_threshold: 0.8 - save_detections: # save samples from the inference results positive_interval: 2 idle_interval: 600 In the configuration excerpt above, there are a few references to variables defined elsewhere in the YAML file. *src_front_door_cam and *src_entry_area_cam are the only references that you have to understand and configure in order to get Ambianic Edge working with your own cameras (or other source of video feed). Here is the definition of this variable reference that you will find in config.yaml: Since camera URIs often contain credentials, we recommended that you store these values in secrets.yaml which needs to be located in the same directory as config.yaml . Ambianid Edge will automatically look for and if available prepend secrets.yaml to config.yaml . The idea here is that you can share config.yaml with others without compromising privacy sensitive parameters. An example valid entry in secretes.yaml for a camera URI, would look like this: secret_uri_front_door_camera: &secret_uri_front_door_camera 'rtsp://user:pass@192.168.86.111:554/Streaming/Channels/101' # add more secret entries as regular yaml mappings The rest of the configuration settings can be left with their default values for now. Notice that each pipeline definition starts with a media source. You can add and remove pipelines to match your home setup. Once you specify the URI of your camera(s), you can start the Docker image.","title":"Configuration"},{"location":"users/quickstart/#starting-ambianic-edge","text":"Here is an example startup line for Ambianic Edge. We will go over each parameter below: # test if coral usb stick is available USB_DIR=/dev/bus/usb if [ -d \"$USB_DIR\" ]; then USB_ARG=\"--device $USB_DIR\" else USB_ARG=\"\" fi # check if there is an image update docker pull ambianic/ambianic-edge:latest # run latest image docker run -it --rm \\ --name ambianic-edge \\ --mount type=bind,source=\"/opt/workspace\",target=/workspace \\ --publish 8778:8778 \\ $USB_ARG \\ ambianic/ambianic:latest Let's review the start script above line by line: # test if coral usb stick is available USB_DIR=/dev/bus/usb if [ -d \"$USB_DIR\" ]; then USB_ARG=\"--device $USB_DIR\" else USB_ARG=\"\" fi Ambianic Edge is able to automatically detect and use the Google Coral EdgeTPU for AI inference. Coral is a powerful USB stick that can speed up inference 5-10 times. However AI inference is only part of all the functions that execute in an Ambianic Edge pipeline. Video decoding and formatting also takes substantial amount of processing time. Overall we've seen Coral to improve performance 30-50% on a realistic Raspberry Pi 4 deployment with multiple simultaneous pipelines pulling images from multiple cameras and processing these images with multiple inference models. If you don't have a Coral device available, no need to worry for now. Raspberry Pi 4 is powerful enough to handle multiple simultaneous camera sourced pipelines with approximately 1-2 frames per second (fps). An objects or person of interest would normally show up in one of your cameras for at leastd one second, which is enough time to be registered and processed by Ambianic Edge on a plain RPI4. Let's look at the other docker image start parameters: docker pull ambianic/ambianic-edge:latest This line ensures that we fetch the latest available Ambianic Edge image. There are weekly semanctic releases with fixes and new updates. Generally this should work for most deployments. Occasionally there may be new releases with breaking changes. Its important to keep an eye on the release notes and make any configuration adjustments if necessary. --name ambianic-edge \\ This parameter just gives a local friendly name of the container that will show up in a docker container listing instead of a cryptic hash tag. --mount type=bind,source=\"/opt/workspace\",target=/workspace \\ This parameter tells Docker to bind your prepared workspace with config.yaml to the image internal /workspace directory. In addition to finding config.yaml , the image will use the workspace directory to write log and data files such as images with object detections. $USB_ARG \\ This is the dynamic parameter which is empty if Coral is not found on the USB bus. Otherwise it tells Docker to let Ambianic Edge access Coral on the usb bus. --publish 8778:8778 \\ This parameter exposes port 8778 where the Ambianic Edge REST API is available. ambianic/ambianic:latest The final parameter points to the latest Ambianic Edge image that Docker will execute. If you use Docker Compose for a more convenient management of multiple docker images, here is a configuration section that you can place in your docker-compose.yaml ambianic: container_name: ambianic restart: unless-stopped privileged: true image: ambianic/ambianic-edge:latest volumes: # - /dev/bus/usb:/dev/bus/usb - /opt/ambianic-edge.prod:/workspace ports: - 8778:8778 restart: on-failure healthcheck: test: [\"CMD\", \"curl\", \"-sI\", \"http://127.0.0.1:8778/\"] interval: 300s timeout: 1s retries: 10 Notice the line with the usb parameter. It would be commented out if there is no Coral attached to your RPI.","title":"Starting Ambianic Edge"},{"location":"users/quickstart/#ambanic-ui-deployment","text":"Ambianic UI is a prorgressive web application (PWA) that provides you with a friendly access to Ambianic Edge device data. For example you can see a timeline view with notable events around your home organized chronologically or by importance. Below is an example timeline screenshot.","title":"Ambanic UI Deployment"},{"location":"users/quickstart/#public-ambanic-ui-app","text":"You can access the public app at https://ui.ambianic.ai/ Due to PWA security requirements it is currently not possible to connect directly from the public app to your local Ambianic Edge device. One solution is to setup a secure connection to your Ambinic Edge device via Dynamic DNS and Open SSH tunnel. However this is an involved process that could take a few hours of your time. There is an alternative solution via WebRTC that only takes a few moments to setup. We are working on it for the next Ambianic UI release. In the meanwhile, you can install Ambianic UI on a local machine - either the same one where Ambianic Edge runs or any other machine that runs on the same LAN. Here is how:","title":"Public Ambanic UI app"},{"location":"users/quickstart/#local-deployment-of-ambianic-ui","text":"Ambianic UI is distributed as a Node JS npm package. https://www.npmjs.com/package/ambianic-ui If you are familiar with Node JS, you can install and run ambianic-ui from its npm distribution. Otherwise, you can follow these steps:","title":"Local deployment of Ambianic UI"},{"location":"users/quickstart/#clone-source-repository","text":"git clone https://github.com/ambianic/ambianic-ui.git cd ambianic-ui","title":"Clone source repository"},{"location":"users/quickstart/#install-dependencies","text":"npm install","title":"Install dependencies"},{"location":"users/quickstart/#compiles-and-hot-reloads-for-development","text":"npm run serve After a few moments you should see a message in your terminal window similar to this: App running at: - Local: http://localhost:8080/ - Network: http://192.168.86.246:8080/ Now you can access the app from your browser. The home screen looks like this: Go to the Settings page to point the UI to the host name of your Ambianic Edge device. You can test the connection to be sure that all is fine. Then navigate to the Timeline page to see detections from your cameras. If you experience problems with your initial setup, feel free to open an issue on github.","title":"Compiles and hot-reloads for development"},{"location":"users/standard-ai-apps/","text":"Standard AI Apps Included with the base Ambianic distribution Object Detection Face Detection Face Recognition Etc.","title":"Standard AI Apps Included with the base Ambianic distribution"},{"location":"users/standard-ai-apps/#standard-ai-apps-included-with-the-base-ambianic-distribution","text":"Object Detection Face Detection Face Recognition Etc.","title":"Standard AI Apps Included with the base Ambianic distribution"},{"location":"users/support/","text":"Support TBD","title":"Support"},{"location":"users/support/#support","text":"TBD","title":"Support"},{"location":"users/webui/","text":"Amianic Web UI Registration and login Securing access AI Apps Overview Pipelines Overview Advanced Pipeline Composition","title":"Web UI"},{"location":"users/webui/#amianic-web-ui","text":"Registration and login Securing access AI Apps Overview Pipelines Overview Advanced Pipeline Composition","title":"Amianic Web UI"}]}